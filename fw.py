# -*- coding: utf-8 -*-
"""FW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/zzalexey/69de07356f2df7877288b22ab012d51e/fw.ipynb

## 1 Загрузите файл HR.csv в pandas dataframe
"""

# Файл данных доступен доступен тут https://drive.google.com/file/d/1INgo03nal-vwFJe7Lec5vOUtOwfJdUr1/view?usp=sharing

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')
hr  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Files/FW/HR.csv')
#hr  = pd.read_csv('https://drive.google.com/file/d/1INgo03nal-vwFJe7Lec5vOUtOwfJdUr1/view?usp=sharing/HR.csv')
hr.head()

hr.info()

"""## 2 Рассчитайте основные статистики для переменных (среднее,медиана,мода,мин/макс,сред.отклонение)."""

hr.describe()

# три последних переменых бинарные(0,1), для них среднее - это доля едениц
# пропущенных значений нет, максимум и минимум выглядят естественно, сторее всего выбросов нет

hr['satisfaction_level'].plot(kind='box',
                   title='satisfaction_level', grid=True)

hr['last_evaluation'].plot(kind='box', grid=True)

hr['number_project'].plot(kind='box', grid=True)

hr['average_montly_hours'].plot(kind='box', grid=True)

hr['time_spend_company'].plot(kind='box', grid=True)

# желательно проверить 6-10 летний стаж, например сравнить с датой образования компании



"""# моды:"""

# моды:
import statistics
for col_name, data in hr.items():
	print("переменная:",col_name, "\nмода:",data.mode())

# разберемся с текстовыми переменными

hr['department'].value_counts()

# категориальная переменная
# рассичтаем моду
mo = statistics.mode(hr['department'])
print(f'Мода: {mo}')

hr['salary'].value_counts()

#порядковая переменная
# дополнительно расчитаем моду
# можно было бы рассчитать медиану, но и она очевидна - medium
mo = statistics.mode(hr['salary'])
print(f'Мода: {mo}')

"""## 3 Рассчитайте и визуализировать корреляционную матрицу для количественных переменных.Определите две самые скоррелированные и две наименее скоррелированные переменные."""

import seaborn as sns
sns.pairplot(hr[['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company']])

hr[['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company']].corr()

# сильных корреляций нет, ряд переменных скорее порядковый, чем непрерывный.
# применим корреляцию Спирмена
hr[['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company']].corr(method='spearman')

sns.heatmap(hr[['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company']].corr(method='spearman'), annot = True)
plt.show()

# Самая высокая корреляция 0.350399: last_evaluation и number_project
# Самая низкая -0.005786: number_project и satisfaction_level
hr.plot(kind='scatter', x='number_project', y='last_evaluation')

import seaborn as sns
df_counts = hr.groupby(['number_project', 'last_evaluation']).size().reset_index(name='counts')

# Draw Stripplot
fig, ax = plt.subplots(figsize=(8,5), dpi= 80)    
sns.scatterplot(df_counts.number_project, df_counts.last_evaluation, size=df_counts.counts*2, ax=ax)
plt.title('График разброса с точками по частоте', fontsize=22)

# те, кто выполнил 6 и 7 проектов, были оценены сравнотельно давно, видимо это "старослужащие", но таких не много
# 2 проекта это в основном сотрудники до 0,6 года

"""## 4 Рассчитайте сколько сотрудников работает в каждом департаменте."""

hr['department'].value_counts()

# альтернативный способ
hr.groupby('department').count()[['last_evaluation']].head(20)

"""## 5 Показать распределение сотрудников по зарплатам. """

hr['salary'].value_counts()

hr['salary'].value_counts().plot(kind='bar', grid=True)

"""## 6 Показать распределение сотрудников по зарплатам в каждомдепартаменте по отдельности"""

gr = hr.groupby(['department','salary']).size().reset_index(name='counts')
gr

gr["salary"] = gr["salary"].str.replace("low", "1 low")
gr["salary"] = gr["salary"].str.replace("medium", "2 low")
gr["salary"] = gr["salary"].str.replace("high", "3 high")
gr_sorted = gr.sort_values(by=['department', 'salary'])
gr_sorted

# в более читаемом виде
table = pd.pivot_table(gr_sorted,
               index=['salary'],
               values=['counts'],
               columns=['department'],
               aggfunc=[np.sum],
               fill_value=0)
table

"""## 7 Проверить гипотезу, что сотрудники с высоким окладом проводят на работе больше времени, чем сотрудники с низким окладом"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from scipy import stats
matplotlib.style.use('ggplot')
# %matplotlib inline
import seaborn as sns

# проверим нормальность распределений
plt.hist((hr['average_montly_hours']), bins=30);

# Явная бимодальность
# посмотрим по отдельности
plt.hist((hr[hr['salary'] == 'low']['average_montly_hours']), bins=30);

plt.hist((hr[hr['salary'] == 'high']['average_montly_hours']), bins=30);

x = hr[hr['salary'] == 'low']['average_montly_hours']
y = hr[hr['salary'] == 'high']['average_montly_hours']
x.name, y.name = 'low salary', 'high salary'    
    
    
x.hist(alpha=0.5, weights=[1./len(x)]*len(x))  
y.hist(alpha=0.5, weights=[1./len(y)]*len(y))
plt.axvline(x.mean(), color='red', alpha=0.8, linestyle='dashed')
plt.axvline(y.mean(), color='blue', alpha=0.8, linestyle='dashed')
plt.legend([x.name, y.name])

# Интересная бимодальность - можно попробовать поискать факторы объясняющие бимодальность. 
# Например, для разных департаментов или числа проектов, но это выходит за рамки задачи.
sns.boxplot(
    x='salary',
    y='average_montly_hours',
    data=hr);

# Визуально видно, что медианы, а скорее всего и средние не отличаются, применим критерий Мана-Уитни, так как нормальность не соблюдена
res = stats.mannwhitneyu(x, y, alternative="two-sided")
# "less", "greater" x>y или x<y
print('p-value: {0}'.format(res[1]))

# Гипотезу о раменстве распределений двух независимых выборок (медиан) не можем отвергнуть.
# Считаем верной гипотезу о равенстве двух независимых выборок (медиан) 
# То есть независимо от оклада сотрудники проводят одинаковое время на работе...вот негодяи!

"""## 8 Рассчитать следующие показатели среди уволившихся и не уволившихся сотрудников (по отдельности):
### Доля сотрудников с повышением за последние 5 лет
### Средняя степень удовлетворенности
### Среднее количество проектов
"""

# number_project
table2 = pd.pivot_table(hr,
               index=['left'],
               values=['promotion_last_5years', 'satisfaction_level','number_project'],
               aggfunc=[np.mean],
               fill_value=0)
print(table2)
table3 = pd.pivot_table(hr,
               index=['left'],
               values=['promotion_last_5years', 'satisfaction_level','number_project'],
               aggfunc=[np.std],
               fill_value=0)
print('\n\n\n')
print(table3)

# среди уволившихся меньше удовлетворенность и доля повышений. 
# По числу проектов в среднем отличий не наблюдается, но станндартное отклонение у уволившихся в 2 раза больше.
# Возможно среди уволившихся есть 2 группы - не интересно работать (малое число выполненых проектов)
# и не оценили работу (большое число выполненых проектов)

"""### 9 Разделить данные на тестовую и обучающую выборки. Построить модель LDA, предсказывающую уволился ли сотрудник на основе имеющихся факторов (кроме department и salary). Оценить качество модели на тестовой выборки"""

#Предположения LDA
#● Нормальное распределение. Предполагается, что анализируемые
#переменные представляют выборку из многомерного нормального
#распределения. Отступление обычно не является критичным. - в данном случае условие не выполняется - есть бинарные переменные
#● Однородность дисперсий/ковариаций. Предполагается, что матрицы
#дисперсий/ковариаций переменных однородны. Как и ранее, малые
#отклонения не фатальны - существенное отклонение
#● Слабая скоррелированность признаков - да, корреляции не высоки

#выполним задачу напрямую, получив максимальную, но возможно сверхпараметрехованную мождель.
#затем рассмотрим выполнение предположений модели и значимость предикторов с цлью сокращения модели
data = hr[['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years']]
data_class = hr['left']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, data_class, test_size=0.25,random_state=88)

print(f'Размер обучающей выбрки: {len(y_train)}')
print(f'Размер тестовой выбрки: {len(y_test)}')

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, lda.predict(X_test))

y_test.value_counts()

# В случае предсказания по моде, мы бы угадали в 2846/3750 = 75,89%;
# А по LDA угадывает в 76% - различия нет. Модель не явлется качественнй;

#pred = pd.DataFrame([y_test,lda.predict(X_test)]).T
#pred.value_counts()
pred = pd.DataFrame([lda.predict(X_test)]).T.value_counts()
pred

# Попробуем масштабировать данные и запустить алгоритм еще разок!
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
#v_1_scaled = scaler.fit_transform(data_cut['sepal length (cm)'].reshape(-1, 1))
#v_1_scaled
#v_2_scaled = scaler.fit_transform(v_2.reshape(-1, 1))
#v_2_scaled
hr_cut_z = scaler.fit_transform(data)
type(hr_cut_z)

df_cut_z = pd.DataFrame(hr_cut_z, columns = ['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years'])
df_cut_z.head()

df_cut_z.describe()

data_class = hr['left']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_cut_z, data_class, test_size=0.25)

print(f'Размер обучающей выбрки: {len(y_train)}')
print(f'Размер тестовой выбрки: {len(y_test)}')

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
from sklearn.metrics import accuracy_score
accuracy_score(y_test, lda.predict(X_test))

# Сила модели немного увеличилась, но все равно осралась на уровне предсказания по модальной категории
# надо рассмотреть другие методы - логистическую регрессию, леревья классификации, перцептрон